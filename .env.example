# chatbot_eval_framework/.env.example
# Copy this file to .env and fill in your actual values.
# DO NOT commit your .env file to version control.

# --- Chatbot Configuration ---
# The API endpoint for your chatbot
CHATBOT_API_ENDPOINT="http://127.0.0.1:5000/chat"
# Optional API key if your chatbot requires authentication (leave blank if not needed)
CHATBOT_API_KEY=""

# --- AWS Bedrock Configuration (Required if using Bedrock for DeepEval metrics) ---
# Your AWS Access Key ID
AWS_ACCESS_KEY_ID=""
# Your AWS Secret Access Key
AWS_SECRET_ACCESS_KEY=""
# The AWS region where your Bedrock models are hosted (e.g., us-east-1, eu-west-1)
AWS_REGION_NAME="us-east-1"

# --- DeepEval Configuration ---
# Optional: Specify a default evaluation model (e.g., a Bedrock model ID like "anthropic.claude-v2")
# If commented out or empty, DeepEval might default to OpenAI (requiring OPENAI_API_KEY)
# or require the model to be specified per-metric in the JSON test case.
# DEEPEVAL_EVALUATION_MODEL="anthropic.claude-v2"
DEEPEVAL_EVALUATION_MODEL=""

# Optional: Run DeepEval evaluations asynchronously (True/False). Default is True.
DEEPEVAL_RUN_ASYNC="True"

# --- Test Execution Configuration ---
# Directory containing the JSON test case files
TEST_DATA_DIR="test_data"
# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL="INFO"

# --- OpenAI Configuration (Required if DeepEval defaults to OpenAI or you use OpenAI models) ---
# OPENAI_API_KEY=""
